@inproceedings{riche19converting,
author = {Rich\'{e}, Taylor L. and Nagle, Jim and Xu, Joyce and Hubbard, Don},
title = {Converting executable floating-point models to executable and synthesizable fixed-point models},
year = {2021},
isbn = {9781728151250},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MODELS-C.2019.00055},
doi = {10.1109/MODELS-C.2019.00055},
abstract = {Execution on field programmable gate arrays (FPGAs) is now necessary for many areas of algorithm development and prototyping, whether it be for the performance that a hardware implementation gives, or the ability to prove an algorithm works, "in the real world."A problem with FPGAs, however, is that the hardware resources are limited. Most algorithm experts design their algorithms using floating-point math which gives flexible precision. Floating point is unfortunately expensive to implement in hardware. Therefore, algorithm designers employ experts in fixed-point math to transform their algorithm to one that will work in hardware, incurring added cost and time to market.We present a novel tool as part of the LabVIEW NXG FPGA Module that uses executable model-driven techniques to guide an algorithm expert to a fixed-point version of their original algorithm model. We walk through a case-study for use of our tool, as well as explain the underlying mathematical and model-driven formalisms on which we build the tool.},
booktitle = {Proceedings of the 22nd International Conference on Model Driven Engineering Languages and Systems Companion},
pages = {354â€“361},
numpages = {8},
location = {Munich, Germany},
series = {MODELS '19 Companion}
}